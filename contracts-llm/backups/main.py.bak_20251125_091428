import os
from pathlib import Path

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
from dotenv import load_dotenv

# ==== Config básica ====

BASE_DIR = Path(__file__).resolve().parent

# Cargar variables desde .env.llm (si existe)
env_path = BASE_DIR / ".env.llm"
if env_path.exists():
    load_dotenv(env_path)

# System prompt para el experto en contratos
SYSTEM_PROMPT_PATH = BASE_DIR / "prompts" / "system_contract_expert.md"
if SYSTEM_PROMPT_PATH.exists():
    SYSTEM_PROMPT = SYSTEM_PROMPT_PATH.read_text(encoding="utf-8")
else:
    SYSTEM_PROMPT = (
        "You are a senior contracts lawyer. Explain clearly, in practical terms, "
        "risks, obligations, negotiation options and alternative wording for clauses. "
        "If something is unclear, say what information is missing."
    )

LLM_PROVIDER = os.getenv("LLM_PROVIDER", "dummy").lower()
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.1"))

app = FastAPI(title="Contracts-AI LLM Backend")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


class AskRequest(BaseModel):
    question: str
    context: str | None = None


class AskResponse(BaseModel):
    answer: str


@app.get("/health")
async def health():
    """Simple healthcheck endpoint."""
    return {"status": "ok", "provider": LLM_PROVIDER}


def build_prompt(question: str, context: str | None) -> str:
    context_text = context or "(no contract text provided)"
    return (
        f"CONTRACT TEXT:\n{context_text}\n\n"
        f"USER QUESTION ABOUT THE CONTRACT:\n{question}\n\n"
        "Answer as an expert contract lawyer. Explain risks, obligations, "
        "negotiation options and, if useful, propose clearer alternative wording. "
        "Be concise and practical."
    )


async def call_lmstudio(prompt: str) -> str:
    api_base = os.getenv("LMSTUDIO_API_BASE", "http://127.0.0.1:1234/v1").rstrip("/")
    url = f"{api_base}/chat/completions"
    model = os.getenv("LLM_MODEL", "lmstudio")

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        "temperature": LLM_TEMPERATURE,
    }

    async with httpx.AsyncClient(timeout=120) as client:
        r = await client.post(url, json=payload)
    try:
        r.raise_for_status()
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"LM Studio error: {e}")

    data = r.json()
    try:
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected LM Studio response: {e}")


async def call_openai(prompt: str) -> str:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="OPENAI_API_KEY is not set")

    api_base = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1").rstrip("/")
    url = f"{api_base}/chat/completions"
    model = os.getenv("LLM_MODEL", "gpt-4.1-mini")

    headers = {
        "Authorization": f"Bearer {api_key}",
    }

    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt},
        ],
        "temperature": LLM_TEMPERATURE,
    }

    async with httpx.AsyncClient(timeout=120, headers=headers) as client:
        r = await client.post(url, json=payload)
    try:
        r.raise_for_status()
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"OpenAI error: {e}")

    data = r.json()
    try:
        return data["choices"][0]["message"]["content"]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected OpenAI response: {e}")


async def call_dummy(prompt: str) -> str:
    """Modo de prueba sin modelo real."""
    return (
        "[DUMMY ANSWER] LLM backend is running but LLM_PROVIDER is set to a "
        "non-configured value or no external model is available.\n\n"
        f"Prompt received:\n{prompt}"
    )


async def generate_answer(question: str, context: str | None) -> str:
    prompt = build_prompt(question, context)

    if LLM_PROVIDER == "lmstudio":
        return await call_lmstudio(prompt)
    elif LLM_PROVIDER == "openai":
        return await call_openai(prompt)
    else:
        return await call_dummy(prompt)


@app.post("/llm/ask", response_model=AskResponse)
async def llm_ask(payload: AskRequest):
    """
    Main endpoint used by the Node backend.

    Body:
    {
        "question": "What is the risk of clause X?",
        "context": "Full or partial contract text..."
    }
    """
    answer = await generate_answer(payload.question, payload.context)
    return AskResponse(answer=answer)


# --- BEGIN /llm/ask endpoint (backwards-compat shim) ---
from typing import Optional
from pydantic import BaseModel
from fastapi import HTTPException

class LlmAskRequest(BaseModel):
    question: str
    context: Optional[str] = None

class LlmAskResponse(BaseModel):
    answer: str

@app.post("/llm/ask", response_model=LlmAskResponse)
async def llm_ask_endpoint(payload: LlmAskRequest) -> LlmAskResponse:
    """
    Generic LLM question endpoint used by the Contracts-AI backend.
    For now this is a safe placeholder so the route exists and never 404s.
    Later we will wire this to the real policy-knowledge RAG pipeline.
    """
    if not payload.question:
        raise HTTPException(status_code=400, detail="Missing 'question' field")

    base_answer = (
        "LLM backend is online, but the /llm/ask logic is not fully wired "
        "to the contract-policy brain yet. "
        f"You asked: {payload.question}"
    )

    if payload.context:
        base_answer += f" | Context length: {len(payload.context)} characters."

    return LlmAskResponse(answer=base_answer)
# --- END /llm/ask endpoint ---

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "4050"))
    uvicorn.run("main:app", host="0.0.0.0", port=port)

