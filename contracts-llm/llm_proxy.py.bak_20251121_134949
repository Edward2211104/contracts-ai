import os
import logging
import requests

logger = logging.getLogger(__name__)

# LM Studio / OpenAI-compatible endpoint (ya lo tienes en 127.0.0.1:1234)
UPSTREAM_LLM_URL = os.getenv("UPSTREAM_LLM_URL", "http://localhost:1234/v1/chat/completions")
UPSTREAM_LLM_MODEL = os.getenv("UPSTREAM_LLM_MODEL", "openai/gpt-oss-20b")
UPSTREAM_LLM_MAX_TOKENS = int(os.getenv("UPSTREAM_LLM_MAX_TOKENS", "1024"))

def _call_lm(payload: dict) -> str:
    """
    Llama al servidor de LM Studio (OpenAI-compatible) y devuelve el contenido del primer mensaje.
    """
    try:
        logger.info("Calling LM Studio at %s", UPSTREAM_LLM_URL)
        resp = requests.post(UPSTREAM_LLM_URL, json=payload, timeout=90)
        resp.raise_for_status()
        data = resp.json()
        choices = data.get("choices", [])
        if not choices:
            logger.error("No choices in LM response: %s", data)
            return "I could not generate an answer because the language model returned an empty response."
        msg = choices[0].get("message", {})
        content = msg.get("content", "").strip()
        return content or "I could not generate an answer because the language model returned no content."
    except Exception as e:
        logger.exception("Error calling LM Studio: %s", e)
        return "There was an internal error calling the local language model. Please try again."

def call_lm_studio(messages):
    """
    Mantengo esta función con el mismo nombre que usa el resto del backend.
    messages debe ser una lista de dicts con {role, content}.
    """
    payload = {
        "model": UPSTREAM_LLM_MODEL,
        "messages": messages,
        "max_tokens": UPSTREAM_LLM_MAX_TOKENS,
        "temperature": 0.2,
    }
    return _call_lm(payload)

def build_contract_messages(question: str, contract_text: str | None, extra_context: str | None = None):
    """
    Construye el bloque de mensajes para preguntas de contratos.
    - Si hay contract_text, lo incluye explícitamente.
    - Si no lo hay, responde basándose en conocimiento general de contratos,
      pero aclarando que no está leyendo un contrato concreto.
    """
    system_base = (
        "You are an expert lawyer in commercial, rental and insurance contracts. "
        "You answer very precisely, step by step, and you always think like a contract drafter. "
        "If you have specific contract text, you MUST base your answer strictly on that text and, "
        "when useful, quote the relevant clauses. "
        "If you do NOT have the actual contract text, you still answer using general knowledge "
        "of typical contracts, but you clearly say that you are not reading the user's exact document."
    )

    if contract_text and contract_text.strip():
        system_prompt = (
            system_base
            + " You have access to the full contract text below. Answer ONLY based on this contract, "
            + "not on generic law.\n\nCONTRACT TEXT:\n"
            + contract_text
        )
    else:
        system_prompt = (
            system_base
            + " You do NOT have the actual contract text for this question. "
            + "Answer using general principles and typical clause structures, and say explicitly "
            + "that you are not reading the exact contract."
        )

    user_parts = [question]
    if extra_context:
        user_parts.append(f"Extra context from the app: {extra_context}")
    user_prompt = "\n\n".join(user_parts)

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

def answer_contract_question(question: str, contract_text: str | None = None, extra_context: str | None = None) -> str:
    """
    Función de alto nivel usada por los endpoints de RAG u otros puntos:
    dado un question y opcionalmente contract_text, devuelve la respuesta del LLM.
    """
    messages = build_contract_messages(question, contract_text, extra_context)
    return call_lm_studio(messages)
